{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BootCamp-BMA/colabs/blob/main/dziriBertRandomSearch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f1rEp7vfr9x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3I0TExNgJmt",
        "outputId": "9f7e0f89-8e57-4279-8394-aea5398e8da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive/MyDrive/arabic_fake_news/FASSILA/cleaned_data.csv\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "gcK8rd5hfskq",
        "outputId": "96ec04d1-3bae-4ef2-e592-a99ae59f6a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9636, 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 9636,\n  \"fields\": [\n    {\n      \"column\": \"news\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9611,\n        \"samples\": [\n          \"\\u064a\\u0627\\u0631\\u0628\\u064a \\u064a\\u0627 \\u0627\\u0644\\u0648\\u0627\\u062a\\u0633 \\u0647\\u0644 \\u064a\\u0639\\u0642\\u0644 \\u0628\\u064a\\u0645\\u0648\\u062a \\u0641\\u064a\\u0631\\u0648\\u0633 \\u0643\\u0648\\u0631\\u0648\\u0646\\u0627 \\u0645\\u0646 \\u0627\\u0644\\u063a\\u0631\\u063a\\u0631\\u0629 \\u0628\\u0627\\u0644\\u0645\\u0644\\u062d \\u0648\\u0627\\u0644\\u0645\\u0648\\u064a\\u0627 \\u0642\\u0631\\u0641 \\u0627\\u0634\\u0627\\u0639\\u0627\\u062a\\u0647 \\u0645\\u0631\\u0647 \\u0642\\u0631\\u0641\",\n          \"\\u0627\\u0644\\u0627\\u0639\\u0635\\u0627\\u0631 \\u0645\\u0627\\u0631\\u064a\\u0627 \\u0645\\u0627 \\u064a\\u0647\\u062f\\u062f\\u0634 \\u0639\\u0642\\u0627\\u0631\\u0627\\u062a \\u0628\\u0648\\u062a\\u064a\\u0646 \\u0627\\u0644\\u0641\\u0627\\u062e\\u0631\\u0629\",\n          \"\\u0637\\u0648\\u0631\\u0648 \\u0644\\u0642\\u0627\\u062d \\u062a\\u0639 \\u0633\\u0631\\u0637\\u0627\\u0646 \\u0644\\u062e\\u0644\\u0627\\u064a\\u0627 \\u0627\\u0644\\u0644\\u064a\\u0641\\u064a\\u0629 \\u0641\\u0639\\u0627\\u0645 2007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-19d98111-049b-46ce-b05a-4467c70b664d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>فلقرن لواحد وعشرين لقاو الدوايات اللي ضد لفيروسات</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>عرف ردود الافعال عربيا وعالميا بعد واش صرا فغزة</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>راه معول مون فكوريا الجنوبية باش يتعاون مع روس...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>تدعو ايرماراحشدا ل الواقعية المملكة المتحدة عل...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>الذهب طلع بدعم من عوامل فنية</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19d98111-049b-46ce-b05a-4467c70b664d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-19d98111-049b-46ce-b05a-4467c70b664d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-19d98111-049b-46ce-b05a-4467c70b664d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d45d22b1-836a-4d0d-b756-5280b2824350\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d45d22b1-836a-4d0d-b756-5280b2824350')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d45d22b1-836a-4d0d-b756-5280b2824350 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                news  label\n",
              "0  فلقرن لواحد وعشرين لقاو الدوايات اللي ضد لفيروسات      1\n",
              "1    عرف ردود الافعال عربيا وعالميا بعد واش صرا فغزة      0\n",
              "2  راه معول مون فكوريا الجنوبية باش يتعاون مع روس...      0\n",
              "3  تدعو ايرماراحشدا ل الواقعية المملكة المتحدة عل...      1\n",
              "4                       الذهب طلع بدعم من عوامل فنية      0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = \"/content/drive/My Drive/arabic_fake_news/FASSILA/cleaned_data.csv\"\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "df.head(5)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds8ce-yfhLiC",
        "outputId": "10c71511-96b2-4607-ad80-d58b93f057c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#######################################\n",
            "Random Search Iteration 23\n",
            "-----------------------------------------------------\n",
            "Latest Best: Acc=0.6805, Params={'batch_size': 8, 'learning_rate': 5e-05, 'dropout': 0.3, 'num_epochs': 3, 'weight_decay': 0.01, 'class_weights': [2.0, 1.0], 'results_path': '/content/drive/My Drive/arabic_fake_news/FASSILA/03_out.csv'}\n",
            "-----------------------------------------------------\n",
            "Trying: Layers={10: True}, Params={'batch_size': 32, 'learning_rate': 1e-05, 'dropout': 0.1, 'num_epochs': 4, 'weight_decay': 0.001, 'class_weights': [2.0, 1.0], 'results_path': '/content/drive/My Drive/arabic_fake_news/FASSILA/03_out.csv'}\n",
            "-----------------------------------------------------\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                        \n",
            "                                                                        \n",
            "                                                                        \n",
            "Epoch 4/4:  37%|███▋      | 90/241 [00:03<00:05, 27.90it/s, loss=0.572]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Generic utility functions\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def load_data(file_path, text_column='news', label_column='label'):\n",
        "    df = pd.read_csv(file_path)\n",
        "    texts = df[text_column].tolist()\n",
        "    labels = df[label_column].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "def tokenize_data(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "def create_dataset(encodings, labels):\n",
        "    return TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels))\n",
        "\n",
        "def split_data(dataset, batch_size, test_size=0.2, num_workers=8):\n",
        "    train_data, test_data = train_test_split(dataset, test_size=test_size, random_state=42)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, persistent_workers=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, pin_memory=True, persistent_workers=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def configure_model(model_name, trainable_layers=None, num_labels=2, dropout=0.1):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    model.dropout = torch.nn.Dropout(dropout)\n",
        "    if trainable_layers:\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'bert.encoder.layer' in name:\n",
        "                layer_num = int(name.split('.')[3])\n",
        "                param.requires_grad = trainable_layers.get(layer_num, False)\n",
        "            elif 'classifier' in name or 'dropout' in name:\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "def setup_training_components(model, train_loader, num_epochs, learning_rate, class_weights, device):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "    num_training_steps = len(train_loader) * num_epochs\n",
        "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "    class_weights = torch.tensor(class_weights, device=device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    return optimizer, lr_scheduler, criterion\n",
        "\n",
        "def train_model(model, train_loader, optimizer, criterion, lr_scheduler, device, num_epochs):\n",
        "    model.to(device)\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        # Use dynamic_ncols and position to ensure proper updating\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", file=sys.stdout, leave=False, dynamic_ncols=True, position=0)\n",
        "        for batch in loop:\n",
        "            input_ids, attention_mask, labels = [x.to(device, non_blocking=True) for x in batch]\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            lr_scheduler.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "        # Refresh the display after each epoch\n",
        "        loop.close()\n",
        "        print()  # Newline to separate epochs cleanly\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_preds, test_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            for batch in test_loader:\n",
        "                input_ids, attention_mask, labels = [x.to(device, non_blocking=True) for x in batch]\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                test_preds.extend(preds.cpu().tolist())\n",
        "                test_labels.extend(labels.cpu().tolist())\n",
        "    return accuracy_score(test_labels, test_preds)\n",
        "\n",
        "def load_or_create_results_df(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        return pd.read_csv(file_path)\n",
        "    return pd.DataFrame(columns=['id', 'execution_time', 'trainable_layers', 'accuracy', 'batch_size', 'learning_rate', 'dropout', 'num_epochs', 'weight_decay'])\n",
        "\n",
        "def check_existing_experiment(df, trainable_layers, batch_size, learning_rate, dropout, num_epochs, weight_decay):\n",
        "    trainable_layers_str = str(trainable_layers) if trainable_layers else \"None\"\n",
        "    mask = (df['trainable_layers'] == trainable_layers_str) & \\\n",
        "           (df['batch_size'] == batch_size) & \\\n",
        "           (df['learning_rate'] == learning_rate) & \\\n",
        "           (df['dropout'] == dropout) & \\\n",
        "           (df['num_epochs'] == num_epochs) & \\\n",
        "           (df['weight_decay'] == weight_decay)\n",
        "    return mask.any()\n",
        "\n",
        "def save_results(df, file_path):\n",
        "    df.to_csv(file_path, index=False)\n",
        "\n",
        "# Simplified initial evaluation without threading\n",
        "def initial_evaluation(model_name, train_loader, test_loader, device, initial_params):\n",
        "    initial_configs = [{i: True} for i in range(12)]  # Single-layer configs\n",
        "    best_accuracy = 0.0\n",
        "    best_layers = None\n",
        "\n",
        "    for layers in initial_configs[:4]:  # Limit to 4 to keep it simple\n",
        "        model = configure_model(model_name, layers, dropout=initial_params['dropout'])\n",
        "        optimizer, lr_scheduler, criterion = setup_training_components(model, train_loader, initial_params['num_epochs'],\n",
        "                                                                      initial_params['learning_rate'], initial_params['class_weights'], device)\n",
        "        trained_model = train_model(model, train_loader, optimizer, criterion, lr_scheduler, device, initial_params['num_epochs'])\n",
        "        accuracy = evaluate_model(trained_model, test_loader, device)\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_layers = layers\n",
        "\n",
        "    return best_layers, best_accuracy\n",
        "\n",
        "# Simplified Hill climbing with enhanced output\n",
        "def hill_climbing_layers(model_name, train_loader, test_loader, device, initial_params, max_time=7200, tolerance=0.01):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initial sequential evaluation\n",
        "    best_layers, best_accuracy = initial_evaluation(model_name, train_loader, test_loader, device, initial_params)\n",
        "    best_layer_count = len(best_layers)\n",
        "    print(f\"Initial best layers: {best_layers}, Accuracy: {best_accuracy:.4f}, Layer count: {best_layer_count}\")\n",
        "\n",
        "    iteration = 0\n",
        "    while time.time() - start_time < max_time:\n",
        "        iteration += 1\n",
        "        current_layers = list(best_layers.keys())\n",
        "        neighbors = []\n",
        "        for layer in range(12):\n",
        "            if layer not in current_layers:\n",
        "                new_layers = best_layers.copy()\n",
        "                new_layers[layer] = True\n",
        "                neighbors.append(new_layers)\n",
        "            elif len(current_layers) > 1:\n",
        "                new_layers = best_layers.copy()\n",
        "                del new_layers[layer]\n",
        "                neighbors.append(new_layers)\n",
        "\n",
        "        random.shuffle(neighbors)\n",
        "        improved = False\n",
        "\n",
        "        for neighbor in neighbors:\n",
        "            if time.time() - start_time >= max_time:\n",
        "                break\n",
        "            exec_start = time.time()\n",
        "\n",
        "            # Clear previous output and print header\n",
        "            clear_output(wait=True)\n",
        "            print(f\"#######################################\")\n",
        "            print(f\"Iteration {iteration}\")\n",
        "            print(\"-----------------------------------------------------\")\n",
        "            print(f\"Latest Best: Layers={best_layers}, Acc={best_accuracy:.4f}, Count={best_layer_count}\")\n",
        "            print(\"-----------------------------------------------------\")\n",
        "            print(f\"Trying: {neighbor}\")\n",
        "            print(\"-----------------------------------------------------\")\n",
        "            print(\"Training:\")\n",
        "\n",
        "            # Configure and train model with epoch-wise display\n",
        "            model = configure_model(model_name, neighbor, dropout=initial_params['dropout'])\n",
        "            optimizer, lr_scheduler, criterion = setup_training_components(model, train_loader, initial_params['num_epochs'],\n",
        "                                                                          initial_params['learning_rate'], initial_params['class_weights'], device)\n",
        "            model.to(device)\n",
        "            scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "            for epoch in range(initial_params['num_epochs']):\n",
        "                model.train()\n",
        "                # Use dynamic_ncols and position for proper progress bar updating\n",
        "                loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{initial_params['num_epochs']}\", file=sys.stdout, leave=False, dynamic_ncols=True, position=0)\n",
        "                for batch in loop:\n",
        "                    input_ids, attention_mask, labels = [x.to(device, non_blocking=True) for x in batch]\n",
        "                    optimizer.zero_grad(set_to_none=True)\n",
        "                    with torch.amp.autocast('cuda'):\n",
        "                        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                        loss = criterion(outputs.logits, labels)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    lr_scheduler.step()\n",
        "                    loop.set_postfix(loss=loss.item())\n",
        "                loop.close()\n",
        "                print()  # Newline to separate epochs\n",
        "\n",
        "            # Evaluate model\n",
        "            accuracy = evaluate_model(model, test_loader, device)\n",
        "            layer_count = len(neighbor)\n",
        "            exec_time = time.time() - exec_start\n",
        "\n",
        "            # Check for improvement\n",
        "            if (accuracy > best_accuracy) or \\\n",
        "               (abs(accuracy - best_accuracy) <= tolerance and layer_count < best_layer_count):\n",
        "                best_accuracy = accuracy\n",
        "                best_layers = neighbor\n",
        "                best_layer_count = layer_count\n",
        "                improved = True\n",
        "                print(f\"\\nIteration {iteration}: New best layers: {best_layers}, Accuracy: {best_accuracy:.4f}, Layer count: {best_layer_count}, Time: {exec_time:.2f}s\")\n",
        "                break\n",
        "\n",
        "        if not improved and iteration > 5:\n",
        "            best_layers = {random.randint(0, 11): True}\n",
        "            print(f\"\\nIteration {iteration}: No improvement. Restarting with: {best_layers}\")\n",
        "            continue\n",
        "        elif not improved:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"#######################################\")\n",
        "            print(f\"Iteration {iteration}\")\n",
        "            print(\"-----------------------------------------------------\")\n",
        "            print(f\"Final Best: Layers={best_layers}, Accuracy={best_accuracy:.4f}, Count={best_layer_count}\")\n",
        "            print(\"-----------------------------------------------------\")\n",
        "            print(\"No improvement found. Stopping.\")\n",
        "            break\n",
        "\n",
        "    return best_layers, best_accuracy\n",
        "\n",
        "# Random search for other parameters with enhanced output\n",
        "def random_search_params(model_name, dataset, device, best_layers, param_space, max_time=3600):\n",
        "    start_time = time.time()\n",
        "    results_df = load_or_create_results_df(param_space['results_path'])\n",
        "    iteration = 0\n",
        "    best_accuracy = 0.0\n",
        "    best_params = None\n",
        "\n",
        "    while time.time() - start_time < max_time:\n",
        "        iteration += 1\n",
        "        params = {\n",
        "            'batch_size': random.choice(param_space['batch_size']),\n",
        "            'learning_rate': random.choice(param_space['learning_rate']),\n",
        "            'dropout': random.choice(param_space['dropout']),\n",
        "            'num_epochs': random.choice(param_space['num_epochs']),\n",
        "            'weight_decay': random.choice(param_space['weight_decay']),\n",
        "            'class_weights': param_space['class_weights'],\n",
        "            'results_path': param_space['results_path']\n",
        "        }\n",
        "\n",
        "        if check_existing_experiment(results_df, best_layers, params['batch_size'], params['learning_rate'],\n",
        "                                     params['dropout'], params['num_epochs'], params['weight_decay']):\n",
        "            continue\n",
        "\n",
        "        train_loader, test_loader = split_data(dataset, params['batch_size'])\n",
        "        exec_start = time.time()\n",
        "\n",
        "        # Clear previous output and print header\n",
        "        clear_output(wait=True)\n",
        "        print(f\"#######################################\")\n",
        "        print(f\"Random Search Iteration {iteration}\")\n",
        "        print(\"-----------------------------------------------------\")\n",
        "        if best_params:\n",
        "            print(f\"Latest Best: Acc={best_accuracy:.4f}, Params={best_params}\")\n",
        "        else:\n",
        "            print(\"Latest Best: None yet\")\n",
        "        print(\"-----------------------------------------------------\")\n",
        "        print(f\"Trying: Layers={best_layers}, Params={params}\")\n",
        "        print(\"-----------------------------------------------------\")\n",
        "        print(\"Training:\")\n",
        "\n",
        "        # Configure and train model with epoch-wise display\n",
        "        model = configure_model(model_name, best_layers, dropout=params['dropout'])\n",
        "        optimizer, lr_scheduler, criterion = setup_training_components(model, train_loader, params['num_epochs'],\n",
        "                                                                      params['learning_rate'], params['class_weights'], device)\n",
        "        model.to(device)\n",
        "        scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "        for epoch in range(params['num_epochs']):\n",
        "            model.train()\n",
        "            # Use dynamic_ncols and position for proper progress bar updating\n",
        "            loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{params['num_epochs']}\", file=sys.stdout, leave=False, dynamic_ncols=True, position=0)\n",
        "            for batch in loop:\n",
        "                input_ids, attention_mask, labels = [x.to(device, non_blocking=True) for x in batch]\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                lr_scheduler.step()\n",
        "                loop.set_postfix(loss=loss.item())\n",
        "            loop.close()\n",
        "            print()  # Newline to separate epochs\n",
        "\n",
        "        # Evaluate model\n",
        "        accuracy = evaluate_model(model, test_loader, device)\n",
        "        execution_time = time.time() - exec_start\n",
        "\n",
        "        # Update best if improved\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_params = params.copy()\n",
        "            print(f\"\\nIteration {iteration}: New best accuracy: {best_accuracy:.4f}, Time: {execution_time:.2f}s, Params={best_params}\")\n",
        "        else:\n",
        "            print(f\"\\nIteration {iteration}: Accuracy: {accuracy:.4f}, Time: {execution_time:.2f}s, Params={params}\")\n",
        "\n",
        "        # Save results\n",
        "        new_row = {\n",
        "            'id': len(results_df) + 1,\n",
        "            'execution_time': execution_time,\n",
        "            'trainable_layers': str(best_layers),\n",
        "            'accuracy': accuracy,\n",
        "            'batch_size': params['batch_size'],\n",
        "            'learning_rate': params['learning_rate'],\n",
        "            'dropout': params['dropout'],\n",
        "            'num_epochs': params['num_epochs'],\n",
        "            'weight_decay': params['weight_decay']\n",
        "        }\n",
        "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "        save_results(results_df, params['results_path'])\n",
        "\n",
        "    # Final summary\n",
        "    clear_output(wait=True)\n",
        "    print(f\"#######################################\")\n",
        "    print(\"Random Search Completed\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(f\"Final Best: Layers={best_layers}, Accuracy={best_accuracy:.4f}, Params={best_params}\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(f\"Total iterations: {iteration}, Total time: {(time.time() - start_time):.2f}s\")\n",
        "\n",
        "def main():\n",
        "    data_path = '/content/drive/My Drive/arabic_fake_news/FASSILA/cleaned_data.csv'\n",
        "    results_path = '/content/drive/My Drive/arabic_fake_news/FASSILA/03_out.csv'\n",
        "    model_name = \"alger-ia/dziribert\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    initial_params = {\n",
        "        'batch_size': 16,\n",
        "        'learning_rate': 2e-5,\n",
        "        'dropout': 0.1,\n",
        "        'num_epochs': 2,\n",
        "        'weight_decay': 0.01,\n",
        "        'class_weights': [2.0, 1.0],\n",
        "        'results_path': results_path\n",
        "    }\n",
        "\n",
        "    param_space = {\n",
        "        'batch_size': [8, 16, 32],\n",
        "        'learning_rate': [1e-5, 2e-5, 5e-5],\n",
        "        'dropout': [0.1, 0.2, 0.3],\n",
        "        'num_epochs': [2, 3, 4],\n",
        "        'weight_decay': [0.001, 0.01, 0.1],\n",
        "        'class_weights': [2.0, 1.0],\n",
        "        'results_path': results_path\n",
        "    }\n",
        "\n",
        "    set_seed()\n",
        "    texts, labels = load_data(data_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    encodings = tokenize_data(texts, tokenizer)\n",
        "    dataset = create_dataset(encodings, labels)\n",
        "    train_loader, test_loader = split_data(dataset, initial_params['batch_size'])\n",
        "\n",
        "    print(\"Starting Hill Climbing for Layers...\")\n",
        "    hill_start = time.time()\n",
        "    best_layers, best_accuracy = hill_climbing_layers(model_name, train_loader, test_loader, device, initial_params, max_time=1*3600)\n",
        "    hill_time = time.time() - hill_start\n",
        "    print(f\"Best layers found: {best_layers}, Accuracy: {best_accuracy:.4f}, Time: {hill_time:.2f}s\")\n",
        "\n",
        "    print(\"Starting Random Search for Other Parameters...\")\n",
        "    random_search_params(model_name, dataset, device, best_layers, param_space, max_time=max(3600, 10800 - hill_time))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_layers"
      ],
      "metadata": {
        "id": "Haxjdw245b_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rPMJ1Cp_5fwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNoYf5ADXvI4CRQUYrQ/SFM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}